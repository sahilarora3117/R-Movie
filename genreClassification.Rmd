---
title: "R J Component"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(dplyr)
library(Hmisc)
library(psych)
library(tidyr)
```

```{r}
movie <- read.csv('movie_metadata.csv', stringsAsFactors = F)
str(movie)
dim(movie)
summary(movie)
```

```{r}
ggplot(aes(x = num_critic_for_reviews), data = movie) + geom_histogram(bins = 20, color = 'white') + ggtitle('Histogram of Number of reviews')
summary(movie$num_critic_for_reviews)
```

The distribution of the number of reviews is right skewed. Among these 5043 movies, the minimum number of review was 1 and the maximum number of reviews was 813. Majority of the movies received less than 200 reviews. 

```{r}
ggplot(aes(x = imdb_score), data = movie) + geom_histogram(bins = 20, color = 'white') + ggtitle('Histogram of Scores')
summary(movie$imdb_score)
```

The score distribution is left skewed, with minimum score at 1.60 and maximum score at 9.50.

```{r}
ggplot(aes(x = title_year), data = movie) + geom_histogram(color='white') +
  ggtitle('Histogram of Title Year')
```

Most of the movies in the dataset were produced after 2000.

```{r}
boxplot(imdb_score ~ title_year, data=movie, col='indianred')
title("IMDB score vs Title year")
```

However, the movies with the highest scores were produced in the 1950s, and there have been significant amount of low score movies came out in the recent years. 

### Which countries produced the most movies and which countries have the highest scores?

```{r}
country_group <- group_by(movie, country)
movie_by_country <- summarise(country_group,
                               mean_score = mean(imdb_score),
                               n = n()) 
ggplot(aes(x = country, y = n, fill = country), data = movie_by_country) + geom_bar(stat = 'identity') + theme(legend.position = "none", axis.text=element_text(size=6)) +
  coord_flip() + ggtitle('Countries vs Number of Movies')
```

The USA produced the most number of movies. 

```{r}
ggplot(aes(x = country, y = mean_score, fill = country), data = movie_by_country) + geom_bar(stat = 'identity') + theme(legend.position = "none", axis.text=element_text(size=7)) +
  coord_flip() + ggtitle('Countries vs IMDB Scores')
```

But that does not mean their movie are all good quality. Kyrgyzstan, Libya and United Arab Emirates might have the highest average scores.

### Multiple Linear Regression - Variable Selection

Time to do something serious work, I intend to predict IMDB scores from the other variables using multiple linear regression model. Because regression can't deal with missing values, I will eliminate all missing values.  

```{r}
movie$imdb_score <- as.numeric(impute(movie$imdb_score, mean))
movie$num_critic_for_reviews <- as.numeric(impute(movie$num_critic_for_reviews, mean))
movie$duration <- as.numeric(impute(movie$duration, mean))
movie$director_facebook_likes <- as.numeric(impute(movie$director_facebook_likes, mean))
movie$actor_3_facebook_likes <- as.numeric(impute(movie$actor_3_facebook_likes, mean))
movie$actor_1_facebook_likes <- as.numeric(impute(movie$actor_1_facebook_likes, mean))
movie$gross <- as.numeric(impute(movie$gross, mean))
movie$cast_total_facebook_likes <- as.numeric(impute(movie$cast_total_facebook_likes, mean))
movie$facenumber_in_poster <- as.numeric(impute(movie$facenumber_in_poster, mean))
movie$budget <- as.numeric(impute(movie$budget, mean))
movie$title_year <- as.numeric(impute(movie$title_year, median))
movie$actor_2_facebook_likes <- as.numeric(impute(movie$actor_2_facebook_likes, mean))
movie$aspect_ratio <- as.numeric(impute(movie$aspect_ratio, mean))
```

```{r}
movie <- movie %>%
  mutate(genres=gsub("[-]","",genres))
movie = movie %>%
  mutate(row = row_number()) %>%
    separate_rows(genres, sep = '\\|') %>%
      pivot_wider(names_from = genres, values_from = genres, values_fn = function(x) 1, values_fill = 0) %>%
        select(-row)

```

```{r}
summary(movie)
#Action + Adventure + Fantasy + Sci-Fi + Thriller + Documentary + Romance + Animation + Comedy + Family + Musical +  Mystery + Western + Drama + History + Sport + Crime + Horror + War + Biography + Music + Game-Show + Reality-TV  + News + Short + Film-Noir
```

Now I have got rid of all 'NA's. And I picked the following variables as potential candidates for the IMDB score predicators.

* num_critic_for_reviews
* duration
* director_facebook_likes
* actor_1_facebook_likes
* gross
* cast_total_facebook_likes
* facenumber_in_poster
* budget 
* movie_facebook_likes

Select a subset of numeric variables for regression modelling.

```{r}
movie_sub <- subset(movie, select = c(3, 4, 5, 8, 9, 13, 15, 22, 27, 28:53, 25))
movie_sub
```

### Construct the model

Split data into training and testing.

```{r}
set.seed(123)
train_size <- 0.8 
train_index <- sample.int(length(movie_sub$imdb_score), length(movie_sub$imdb_score) * train_size)
train_sample <- movie_sub[train_index,]
test_sample <- movie_sub[-train_index,]
```

### Fit the model 

I am trying out a stepwise selection of variables by backwards elimination. So I start with all candidate varibles and elimiate one at a time.

```{r}
fit <- lm(imdb_score ~ num_critic_for_reviews + duration +    director_facebook_likes + actor_1_facebook_likes + gross + cast_total_facebook_likes + facenumber_in_poster + budget + movie_facebook_likes + Action + Adventure + Fantasy +  Thriller + Documentary + Romance + Animation + Comedy + Family + Musical +  Mystery + Western + Drama + History + Sport + Crime + Horror + War + Biography + Music + News + Short, data=train_sample)
summary(fit) 
```

I am going to eliminate the variables that has little value, - gross and budget, one at a time, and fit it again.

```{r}
fit <- lm(imdb_score ~ num_critic_for_reviews + duration + budget +   director_facebook_likes + actor_1_facebook_likes + cast_total_facebook_likes + facenumber_in_poster + movie_facebook_likes + Action + Adventure + Fantasy +  Thriller + Documentary + Romance + Animation + Comedy + Family + Musical +  Mystery + Western + Drama + History + Sport + Crime + Horror + War + Biography + Music + News + Short, data=train_sample)
summary(fit) 
```

```{r}
fit <- lm(imdb_score ~ num_critic_for_reviews + duration +   director_facebook_likes + actor_1_facebook_likes + cast_total_facebook_likes + facenumber_in_poster + movie_facebook_likes + Action + Adventure + Fantasy +  Thriller + Documentary + Romance + Animation + Comedy + Family + Musical +  Mystery + Western + Drama + History + Sport + Crime + Horror + War + Biography + Music + News + Short, data=train_sample)
summary(fit) 
```

From the fitted model, I find that the model is significant since the p-value is very small. The "cast_total_facebook_likes" and "facenumber_in_poster" has negative weight. This model has multiple R-squared score of 0.143, meaning that around 14.3% of the variability can be explained by this model.

Let me make a few plots of the model I arrived at.

```{r}
plot(fit)
```

If I consider IMDB scores of all movies in the dataset, it is a non-linear fit, it has a small degree of nonlinearity.

This charts shows how all of the examples of residuals compare against theoretical distances from the model. I can see I have a bit problems here because some of the observations are not neatly fit the line. 

This chart shows the distribution of residuals around the linear model in relation to the IMDB scores of all movies in my data. The higher the score, the less movies, and most movies are in the lower or median score range.

This chart identifies all extrme values, but I don't see any extrme value has huge impact on my model. 

At this point, I think this model is as good as I can get. Let's evaluate it. 

```{r}
train_sample$pred_score <- predict(fit, newdata = subset(train_sample, select=c(imdb_score, num_critic_for_reviews, duration, director_facebook_likes, actor_1_facebook_likes, cast_total_facebook_likes, facenumber_in_poster, movie_facebook_likes, Action ,Adventure, Fantasy,Thriller, Documentary ,Romance ,Animation ,Comedy ,Family ,Musical ,Mystery, Western ,Drama ,History ,Sport ,Crime ,Horror,War ,Biography ,Music, News ,Short)))
test_sample$pred_score <- predict(fit, newdata = subset(test_sample, select=c(imdb_score, num_critic_for_reviews, duration, director_facebook_likes, actor_1_facebook_likes, cast_total_facebook_likes, facenumber_in_poster, movie_facebook_likes, Action ,Adventure, Fantasy,Thriller, Documentary ,Romance ,Animation ,Comedy ,Family ,Musical ,Mystery, Western ,Drama ,History ,Sport ,Crime ,Horror,War ,Biography ,Music, News ,Short)))
```

The theoretical model performance is defined here as R-Squared

```{r}
summary(fit)
```

Check how good the model is on the training set.

```{r}
train_corr <- round(cor(train_sample$pred_score, train_sample$imdb_score), 2)
train_rmse <- round(sqrt(mean((train_sample$pred_score - train_sample$imdb_score)^2)))
train_mae <- round(mean(abs(train_sample$pred_score - train_sample$imdb_score)))
c(train_corr^2, train_rmse, train_mae)
```

The correlation between predicted score and actual score for the training set is 14.44%, which is cery close to theoretical R-Squared for the model, this is good news. However, on average, on the set of the observations I have previously seen, I am going to make 1 score difference when estimating. 

Check how good the model is on the test set.

```{r}
test_corr <- round(cor(test_sample$pred_score, test_sample$imdb_score), 2)
test_rmse <- round(sqrt(mean((test_sample$pred_score - test_sample$imdb_score)^2)))
test_mae <- round(mean(abs(test_sample$pred_score - test_sample$imdb_score)))
c(test_corr^2, test_rmse, test_mae)
```

```{r}
test_corr <- round(cor(test_sample$pred_score, test_sample$imdb_score), 2)
test_rmse <- round(sqrt(mean((test_sample$pred_score - test_sample$imdb_score)^2)))
test_mae <- round(mean(abs(test_sample$pred_score - test_sample$imdb_score)))
c(test_corr^2, test_rmse, test_mae)
```

## Decision tree

```{r}
library(rpart)
library(rpart.plot)
```

```{r}
movie_sub_dectre <- subset(movie, select = c(3, 4, 5, 8, 9, 13, 15, 22, 27, 28:53))
movie_sub_dectre$imdb_cat <- as.numeric(cut2(movie_sub$imdb_score, g=3))

movie_sub_dectre$imdb_cat <- factor(x=movie_sub_dectre$imdb_cat, levels=sort(unique(movie_sub_dectre$imdb_cat)), labels = c("Low", "Medium", "High"))
movie_sub_dectre
```

```{r}
set.seed(123)
train_size <- 0.8 
train_index <- sample.int(length(movie_sub_dectre$imdb_cat), length(movie_sub_dectre$imdb_cat) * train_size)
train_sample <- movie_sub_dectre[train_index,]
test_sample <- movie_sub_dectre[-train_index,]
```

```{r}
fit <- rpart(imdb_cat ~ num_critic_for_reviews + duration + budget + director_facebook_likes + actor_1_facebook_likes + cast_total_facebook_likes + facenumber_in_poster + movie_facebook_likes + Action + Adventure + Fantasy +  Thriller + Documentary + Romance + Animation + Comedy + Family + Musical +  Mystery + Western + Drama + History + Sport + Crime + Horror + War + Biography + Music + News + Short, data=train_sample, method = 'class')
rpart.plot(fit)
```

```{r}
fit2 <- rpart(imdb_cat ~ num_critic_for_reviews + duration + budget + director_facebook_likes + actor_1_facebook_likes + cast_total_facebook_likes + facenumber_in_poster + movie_facebook_likes + Action + Adventure + Fantasy +  Thriller + Documentary + Romance + Animation + Comedy + Family + Musical +  Mystery + Western + Drama + History + Sport + Crime + Horror + War + Biography + Music + News + Short, data=test_sample, method = 'class')

predicted <- predict(fit2, test_sample, type = 'class')
table_mat <- table(test_sample$imdb_cat, predicted)
table_mat
```

```{r}
table(test_sample$imdb_cat)
```

```{r}
table(predicted)
```


```{r}
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
print(paste('Accuracy for test', accuracy_Test))
```

## Neural networks

```{r}
#install.packages("neuralnet")
library(neuralnet)
```

```{r}
movie_sub_dectre$imdb_num <- factor(x=movie_sub_dectre$imdb_cat, levels=sort(unique(movie_sub_dectre$imdb_cat)), labels = c(1,2,3))
movie_sub_dectre$imdb_num = as.numeric(movie_sub_dectre$imdb_num)

movie_idk = select(movie_sub_dectre, -c("imdb_cat"))
```

```{r}
index <- sample(1 : nrow(movie_idk),
                round(0.75 * nrow(movie_idk)))
maxs <- apply(movie_idk, 2, max)
mins <- apply(movie_idk, 2, min)
scaled <- as.data.frame(scale(movie_idk,
                              center = mins,
                              scale = maxs - mins))
aa = movie_idk[index, ]
ab = movie_idk[-index, ]

train_ <- scaled[index, ]
test_ <- scaled[-index, ]

train_$imdb_num = as.factor(aa$imdb_num)
test_$imdb_num = as.factor(ab$imdb_num)
```

```{r}
NN = neuralnet(imdb_num ~ num_critic_for_reviews + duration + director_facebook_likes + actor_1_facebook_likes + cast_total_facebook_likes + facenumber_in_poster + movie_facebook_likes, data=train_, hidden = 2)
plot(NN)
```

```{r}
NN = neuralnet(imdb_num ~ num_critic_for_reviews + duration + director_facebook_likes + actor_1_facebook_likes + cast_total_facebook_likes + facenumber_in_poster + movie_facebook_likes, data=test_, hidden = 2)
plot(NN)
```

## Naive bayees geners only

```{r}
library("e1071")

genre_sub <- subset(movie_sub_dectre, select = c(10:35, 36))

index <- sample(1 : nrow(genre_sub), round(0.75 * nrow(genre_sub)))

train_cl <- genre_sub[index, ]
test_cl <- genre_sub[-index, ]

set.seed(120)  
classifier_cl <- naiveBayes(imdb_cat ~ ., data = train_cl)
classifier_cl
```

```{r}
y_pred <- predict(classifier_cl, newdata = test_cl)

cm <- table(test_cl$imdb_cat, y_pred)
cm
 
confusionMatrix(cm)
```